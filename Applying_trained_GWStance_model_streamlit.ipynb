{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP93SoePdtmwt49Fbmn1b5H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaalexlit/omdena_climate_change_challenge_notebooks/blob/main/Applying_trained_GWStance_model_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6zThdeoh8UoQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "## we need this to be able to download big-ish files from GDrive\n",
        "!pip3 install --upgrade gdown\n",
        "!pip install nltk transformers streamlit\n",
        "# Download the model\n",
        "!gdown https://drive.google.com/uc?id=12rVg_bpuDfZbdWRtEN2Jf6SNyMEnax76\n",
        "# Clone GWStance repo \n",
        "!git clone https://github.com/yiweiluo/GWStance.git\n",
        "# Untar the model\n",
        "!tar -xvzf final_model.tar.gz\n",
        "# Install virtualenv and all the requirements\n",
        "!pip install virtualenv\n",
        "!virtualenv gwstance\n",
        "!source /content/gwstance/bin/activate; pip install transformers scipy pandas matplotlib scikit-learn tqdm tensorboard boto3 torch torchvision\n",
        "# copy things for transformers\n",
        "!cp -r /content/GWStance/3_stance_detection/2_Stance_model/for_transformers/* /content/gwstance/lib/python3.9/site-packages/transformers\n",
        "!pip install Dbias\n",
        "!pip install https://huggingface.co/d4data/en_pipeline/resolve/main/en_pipeline-any-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile GWStance_prediction.py\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, pipeline, RobertaForSequenceClassification\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import torch\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "from Dbias.bias_classification import *\n",
        "\n",
        "\n",
        "label_mapping = {0: 'disagree', 1: 'neutral', 2: 'agree'}\n",
        "\n",
        "def is_about_climate(texts, model, tokenizer):\n",
        "    if torch.cuda.is_available():\n",
        "      device = 0\n",
        "      batch_size = 128\n",
        "    else:\n",
        "      device = -1\n",
        "      batch_size = 1\n",
        "    pipe = pipeline(\"text-classification\", model=model,\n",
        "                    tokenizer=tokenizer, device=device,\n",
        "                    truncation=True, padding=True)\n",
        "    labels = []\n",
        "    probs = []\n",
        "    for out in pipe(texts, batch_size=batch_size):\n",
        "        labels.append(out['label'])\n",
        "        probs.append(out['score'])\n",
        "    torch.cuda.empty_cache()\n",
        "    return labels, probs\n",
        "\n",
        "\n",
        "def filter_climate_related(sentences, model, tokenizer):\n",
        "    labels, _ = is_about_climate(sentences, model, tokenizer)\n",
        "    return [doc for label, doc in zip(labels, sentences) if label == 'Yes']\n",
        "\n",
        "def predict_climate_relatedness(sentences, model, tokenizer):\n",
        "  labels, probs = is_about_climate(sentences, model, tokenizer)\n",
        "  df_cli = pd.DataFrame(zip(sentences, labels, probs))\n",
        "  return df_cli\n",
        "\n",
        "def predict_bias_with_dbias(sentences):\n",
        "  print(\"before\")\n",
        "  res = classifier(sentences)\n",
        "  print(\"after\")\n",
        "  for sent, d in zip(sentences, res):\n",
        "    d['sent'] = sent\n",
        "  return pd.DataFrame(res)\n",
        "\n",
        "    \n",
        "def predict_gw_stance(input_sentences, model, tokenizer):\n",
        "  sentences = filter_climate_related(input_sentences, model, tokenizer)\n",
        "\n",
        "  if not sentences:\n",
        "    print(\"No climate related sentences found in the text\")\n",
        "    return None\n",
        "  df = pd.DataFrame(sentences)\n",
        "  df[\"lab\"]= \"neutral\"\n",
        "  df[\"weight\"]= 1.0\n",
        "  df.to_csv('test.tsv', sep='\\t', index=False, header=False)\n",
        "\n",
        "  subprocess.run(\"python /content/GWStance/3_stance_detection/2_Stance_model/predict.py \\\n",
        "  /content/final_model/config.json \\\n",
        "  /content/final_model/no-dev \\\n",
        "  --data-dir /content/ \\\n",
        "  --transformers-dir /content/gwstance/lib/python3.9/site-packages/transformers\".split())\n",
        "\n",
        "  input_df = pd.read_csv(\"/content/test.tsv\", sep='\\t', header=None, names=[\"text\", \"fake1\", \"fake2\"])\n",
        "  preds_df = pd.read_csv(\"/content/final_model/no-dev/predictions_test.tsv\", sep='\\t')\n",
        "\n",
        "  res_df = input_df.join(preds_df)[[\"text\", \"predicted\"]]\n",
        "\n",
        "  res_df['predicted'] = res_df['predicted'].apply(lambda x: label_mapping[x])\n",
        "\n",
        "  return res_df\n",
        "\n",
        "@st.cache_resource\n",
        "def download_models():\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    # make dbias download the models\n",
        "    classifier([\"test\"])\n",
        "    # Load the pre-trained model\n",
        "    model = RobertaForSequenceClassification.from_pretrained('kruthof/climateattention-10k-upscaled',num_labels=2)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"climatebert/distilroberta-base-climate-f\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# Create the Streamlit app\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"Applying trained Global Warming stance model\", \n",
        "                       page_icon=\":earth_americas:\",\n",
        "                       layout='wide')\n",
        "    model, tokenizer = download_models()\n",
        "\n",
        "    # Add a sidebar with links\n",
        "    st.sidebar.title(\"Omdena, Local Chapter, ðŸ‡©ðŸ‡ª Cologne\")\n",
        "    project_link = '[Project Description](https://omdena.com/chapter-challenges/detecting-bias-in-climate-reporting-in-english-and-german-language-news-media/)'\n",
        "    st.sidebar.markdown(project_link, unsafe_allow_html=True)\n",
        "    github_link = '[Github Repo](https://github.com/OmdenaAI/cologne-germany-reporting-bias/)'\n",
        "    st.sidebar.markdown(github_link, unsafe_allow_html=True)\n",
        "\n",
        "    st.header(\"Applying trained Global Warming stance model to the sentences extracted from climate news articles\")\n",
        "    \n",
        "    tab_bias_detection, tab_how_to, tab_faq = st.tabs([\"Global Warming Stance Detection\", \"How-To\", \"FAQ\"])\n",
        "\n",
        "    with tab_bias_detection:\n",
        "      \n",
        "      st.write(\"\"\"Enter a Text below and click the Classify Button \n",
        "      to extract change related claim sentences from text and classify them\n",
        "      as agreeing with Global warming, disagreeing with Global Warming or neutral\"\"\")\n",
        "\n",
        "      text_input = st.text_area(\"Enter Text\")\n",
        "      input_sentences = sent_tokenize(text_input)\n",
        "\n",
        "      # Classify text and show result\n",
        "      if st.button(\"Detect Global Warming stance in climate related sentences\"):\n",
        "        with st.spinner(text='Performing stance detection'):\n",
        "          res = predict_gw_stance(input_sentences, model, tokenizer)\n",
        "          if res is not None:\n",
        "            st.dataframe(res, use_container_width=True)\n",
        "          else:\n",
        "            st.warning(\"None of the extracted sentences are climate related.\")\n",
        "\n",
        "      if st.button(\"Classify sentences from text\"):\n",
        "        with st.spinner(text='Classifying sentences as climate-related or not'):\n",
        "          res = predict_climate_relatedness(input_sentences, model, tokenizer)\n",
        "          st.dataframe(res, use_container_width=True)\n",
        "\n",
        "\n",
        "      if st.button(\"Classify sentences as biased using Dbias library\"):\n",
        "        with st.spinner(text='Classifying sentences as biased'):\n",
        "          res = predict_bias_with_dbias(input_sentences)\n",
        "          st.dataframe(res, use_container_width=True)\n",
        "    \n",
        "    with tab_how_to:\n",
        "      st.write(\"tbd\")\n",
        "    \n",
        "    with tab_faq:\n",
        "      st.write(\"tbd\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hzaiNE-Vowy",
        "outputId": "d728defc-cb5f-439e-bd35-a742c8e240fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting GWStance_prediction.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run streamlit \n",
        "!streamlit run GWStance_prediction.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXF9F1QnXTo4",
        "outputId": "80bab387-654a-41ef-a180-2632f777676f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tVYVmLoZXWem"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}